import json
import yaml
from pathlib import Path
import threading
import queue
import re
from typing import List
import asyncio
from my_tts.audio_player import AudioPlayer
from my_asr.audio_record import AudioRecord
from chat_handler.chat_context_manager import ChatContextManager


class ChatTTSHandler:
    def __init__(self, openai_engine, mcp_client, tts_engine, asr_engine,
                 whitelist_path=None, max_context_tokens=64000, system_role="ai_assistant"):
        # LLMç›¸å…³ç»„ä»¶
        self.llm = openai_engine
        self.mcp_client = mcp_client
        self.system_role = system_role

        # TTSç›¸å…³ç»„ä»¶
        self.tts_engine = tts_engine
        self.audio_player = AudioPlayer()

        # ASRç›¸å…³ç»„ä»¶
        self.asr_engine = asr_engine
        self.audio_recorder = AudioRecord()

        # çº¿ç¨‹é€šä¿¡é˜Ÿåˆ—
        #  ç”¨æˆ·è¾“å…¥inputï¼›llmè¯»å–inputå¹¶è¾“å‡ºåˆ°messageï¼›ttsè¯»å–messagesä¸­çš„å¥å­å¹¶è½¬æ¢ä¸ºéŸ³é¢‘è¾“å‡ºåˆ°audio
        self.input_queue = queue.Queue()
        self.message_queue = queue.Queue()
        self.audio_queue = queue.Queue()

        # çº¿ç¨‹æ§åˆ¶
        self.llm_thread = None
        self.audio_thread = None
        self.should_stop = threading.Event()

        # å·¥å…·åˆ—è¡¨
        self.tools = None
        # å·¥å…·ç™½åå•é…ç½®
        self.tools_whitelist = self._load_tools_whitelist(whitelist_path)
        
        # ä¸Šä¸‹æ–‡
        self.history = []
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context_manager = ChatContextManager(
            llm_engine=openai_engine,
            max_context_tokens=max_context_tokens
        )
        # å½“å‰ç´¯ç§¯messageçš„tokensæ•°é‡ï¼ˆç›´æ¥ä»æœ¬è½®å¯¹è¯çš„responseçš„prompt_tokenä¸­è¯»å–ï¼‰
        self.message_tokens = 0
        # ç´¯ç§¯ä½¿ç”¨çš„tokensè®¡æ•°ï¼ˆå°†æ¯è½®å¯¹è¯çš„tokenséƒ½åŠ èµ·æ¥ï¼‰
        self.tokens_used = {"prompt_cached_tokens": 0, "prompt_miss_tokens": 0, "prompt_tokens": 0,
                            "completion_tokens": 0}

        # websocketï¼šç”¨äºå‘é€éŸ³é¢‘åˆ°å®¢æˆ·ç«¯
        self.websocket = None

    # åœ¨ ChatHandler ç±»ä¸­æ›´æ–° initialize æ–¹æ³•
    async def initialize(self, system_role_path=None):
        """
        åˆå§‹åŒ–èŠå¤©å¤„ç†å™¨ï¼Œå‡†å¤‡å¿…è¦çš„å·¥å…·å’Œä¸Šä¸‹æ–‡ã€‚
        """
        # å‡†å¤‡å·¥å…·åˆ—è¡¨
        self.tools = await self.prepare_tools()
        # åˆå§‹åŒ–ç³»ç»Ÿæç¤º
        self.init_system_prompt(system_role_path)

    def init_system_prompt(self, system_role_path: str):
        if system_role_path:
            # åŠ è½½ YAML é…ç½®æ–‡ä»¶
            with open("chat_handler/system_role_prompt.yaml", "r") as file:
                config = yaml.safe_load(file)

            # è·å–è§’è‰²çš„æè¿°
            role_description = config['roles'][self.system_role]
            tools_description = config['tools']['mcp_tools']
            format_description = config['format']['Chinese']
            total_description = f"{role_description}\n{tools_description}\n{format_description}"
            print(total_description)
            self.history = [{"role": "system", "content": total_description}]

    def split_sentences(self, text: str) -> (List[str], str):
        """
        å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­ï¼Œè¿”å›å®Œæ•´çš„å¥å­åˆ—è¡¨å’Œå¯èƒ½çš„æœªå®Œæˆå¥å­
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¥å­ç»“æŸæ ‡è®°
        sentence_ends = re.finditer(r'[.!?ã€‚ï¼ï¼Ÿ\n]+', text)

        last_end = 0
        sentences = []

        for match in sentence_ends:
            end = match.end()
            sentence = text[last_end:end].strip()
            if sentence:
                sentences.append(sentence)
            last_end = end

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„å¥å­
        remaining = text[last_end:].strip()

        return sentences, remaining

    # llmçº¿ç¨‹-------------------------------------------------------------------------------------------
    async def llm_worker(self):
        """
        LLMå¤„ç†çº¿ç¨‹çš„å·¥ä½œå‡½æ•°
        """
        print("[LLM Thread] å¯åŠ¨")
        while not self.should_stop.is_set():
            try:
                # éé˜»å¡è·å–è¾“å…¥ï¼Œè¶…æ—¶åç»§ç»­å¾ªç¯æ£€æŸ¥ should_stop æ ‡å¿—
                user_input = self.input_queue.get(timeout=0.5)

                # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å†å²è®°å½•
                self.history.append({"role": "user", "content": user_input})

                # llmå¾ªç¯å¤„ç†å½“å‰è¾“å…¥ï¼Œç›´åˆ°æ²¡æœ‰å·¥å…·è°ƒç”¨ä¸ºæ­¢
                while True:
                    # è°ƒç”¨LLM API
                    response_message, finish_reason, tokens_used = await self._call_llm_stream()

                    # æ›´æ–°tokenè®¡æ•°å™¨
                    if tokens_used:
                        self.message_tokens = getattr(tokens_used, 'prompt_tokens', 0)
                        self.tokens_used['prompt_cached_tokens'] += getattr(tokens_used, 'prompt_cache_hit_tokens', 0)
                        self.tokens_used['prompt_miss_tokens'] += getattr(tokens_used, 'prompt_cache_miss_tokens', 0)
                        self.tokens_used['prompt_tokens'] += getattr(tokens_used, 'prompt_tokens', 0)
                        self.tokens_used['completion_tokens'] += getattr(tokens_used, 'completion_tokens', 0)

                    # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯
                    if tokens_used:
                        print(
                            f"\nCurrent tokens used: [prompt_cached_tokens]: {getattr(tokens_used, 'prompt_cache_hit_tokens', 0)}"
                            f" - [prompt_miss_tokens]: {getattr(tokens_used, 'prompt_cache_miss_tokens', 0)}"
                            f" - [prompt_tokens]: {getattr(tokens_used, 'prompt_tokens', 0)}"
                            f" - [completion_tokens]: {getattr(tokens_used, 'completion_tokens', 0)}")
                        print(f"Total tokens used: [prompt_cached_tokens]: {self.tokens_used['prompt_cached_tokens']}"
                              f" - [prompt_miss_tokens]: {self.tokens_used['prompt_miss_tokens']}"
                              f" - [prompt_tokens]: {self.tokens_used['prompt_tokens']}"
                              f" - [completion_tokens]: {self.tokens_used['completion_tokens']}")

                    # 3.1 å¦‚æœLLMæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ ‡è®°å½“å‰å¯¹è¯æ¶ˆæ¯æµå®Œæˆï¼Œå¹¶ä¸”ç²¾ç®€æ¶ˆæ¯
                    if finish_reason != "tool_calls":
                        self.history.append({
                            "role": "assistant",
                            "content": response_message.content,
                        })

                        # æ ‡è®°æ¶ˆæ¯æµå·²å®Œæˆï¼ˆä¸»çº¿ç¨‹å°±ä¼šç»“æŸæ­¤è½®å¯¹è¯ï¼‰
                        self.message_queue.put(None)
                        # æ ‡è®°è¾“å…¥é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ
                        self.input_queue.task_done()

                        # ç²¾ç®€æ¶ˆæ¯
                        self.history = await self.context_manager.manage_context(self.history, self.message_tokens)
                        break

                    # 3.2 å¦åˆ™ï¼ŒLLMè¯·æ±‚å·¥å…·è°ƒç”¨
                    print("ğŸ¤– LLM requested tool calls...")
                    # i. å°†LLMçš„å›å¤æ·»åŠ åˆ°å†å²è®°å½•
                    self.history.append({
                        "role": "assistant",
                        "content": response_message.content,
                        # è¿™é‡Œä¸€å®šè¦æœ‰ tool_calls å­—æ®µï¼Œé‡Œé¢åŒ…å«äº†LLMè¯·æ±‚çš„æ‰€æœ‰å·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯ï¼Œ
                        #  å¦åˆ™ä¸‹é¢è°ƒç”¨å®Œå·¥å…·æ”¾å…¥message çš„toolå­—æ®µä¸­ä¼šå‡ºé”™
                        "tool_calls": response_message.tool_calls
                    })

                    # ii. æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                    await self._process_tool_calls(response_message.tool_calls)

                    # å¸¦ç€å·¥å…·è°ƒç”¨çš„ç»“æœå†æ¬¡è¯·æ±‚LLMè¿›è¡Œæ€»ç»“ï¼Œå¾ªç¯ç»§ç»­
                    print("ğŸ”„ Sending tool results back to LLM for final response...")

            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                print(f"[LLM Thread] é”™è¯¯: {e}")
                self.message_queue.put(f"å¤„ç†é”™è¯¯: {str(e)}")
                self.message_queue.put(None)  # æ ‡è®°å®Œæˆ
                self.input_queue.task_done()

        print("[LLM Thread] å…³é—­")

    async def _call_llm_stream(self):
        """
        è°ƒç”¨LLM APIçš„æµå¼æ–¹å¼
        """
        stream = await self.llm.chat_stream(self.history, self.tools)

        # ç”¨äºç´¯ç§¯å®Œæ•´å“åº”
        response_content = ""
        tool_calls = []
        finish_reason = None
        tokens_used = None

        # å¤„ç†æµå¼å“åº”ï¼ˆä¸éœ€è¦printè¾“å‡ºï¼Œä¸»çº¿ç¨‹ä¼šè¾“å‡ºï¼‰
        # print("LLM: ", end="", flush=True)
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta:
                delta = chunk.choices[0].delta

                # æ”¶é›†å†…å®¹ç‰‡æ®µ
                if delta.content:
                    response_content += delta.content
                    # æµå¼è¾“å‡º
                    # print(delta.content, end="", flush=True)
                    # åŒæ—¶è¿˜å°†å†…å®¹ç‰‡æ®µæ”¾å…¥åˆ°æ¶ˆæ¯é˜Ÿåˆ—
                    self.message_queue.put(delta.content)

                # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
                if delta.tool_calls:
                    for tool_call_delta in delta.tool_calls:
                        # æŸ¥æ‰¾æˆ–åˆ›å»ºå·¥å…·è°ƒç”¨
                        existing_call = next((tc for tc in tool_calls if tc.index == tool_call_delta.index), None)
                        if existing_call is None:
                            tool_calls.append(tool_call_delta)
                        else:
                            # æ›´æ–°ç°æœ‰å·¥å…·è°ƒç”¨
                            if tool_call_delta.function and tool_call_delta.function.name:
                                existing_call.function.name = tool_call_delta.function.name
                            if tool_call_delta.function and tool_call_delta.function.arguments:
                                existing_call.function.arguments = (
                                                                           existing_call.function.arguments or "") + tool_call_delta.function.arguments
                            if tool_call_delta.id:
                                existing_call.id = tool_call_delta.id

            # æ£€æŸ¥å®ŒæˆåŸå› 
            if chunk.choices and chunk.choices[0].finish_reason:
                finish_reason = chunk.choices[0].finish_reason
                tokens_used = chunk.usage

        # print()  # æ¢è¡Œ

        # æ„é€ å“åº”æ¶ˆæ¯
        response_message = type('obj', (object,), {
            'content': response_content,
            'tool_calls': tool_calls if tool_calls else None
        })

        return response_message, finish_reason, tokens_used


    # éŸ³é¢‘æ’­æ”¾çº¿ç¨‹--------------------------------------------------------------------------------------
    async def audio_worker(self):
        """
        éŸ³é¢‘æ’­æ”¾çº¿ç¨‹çš„å·¥ä½œå‡½æ•°
        å¦‚æœå¯¹è¯ä¸åœæ­¢ï¼Œè¯¥çº¿ç¨‹ä¸€ç›´å­˜åœ¨ï¼Œå³ä¸€ç›´å¤„ç†å¯¹è¯
        """
        print("[Audio Thread] å¯åŠ¨")
        while not self.should_stop.is_set():
            try:
                # éé˜»å¡è·å–éŸ³é¢‘æ•°æ®
                audio_data = self.audio_queue.get(timeout=0.5)

                # æ’­æ”¾éŸ³é¢‘
                self.audio_player.play_audio(audio_data)

                # æ ‡è®°éŸ³é¢‘é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ
                self.audio_queue.task_done()
            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                print(f"[Audio Thread] é”™è¯¯: {e}")

        print("[Audio Thread] å…³é—­")


    async def start(self, system_role_path=None, websocket=None):
        """å¯åŠ¨å¤„ç†å™¨ï¼Œåˆå§‹åŒ–å¹¶åˆ›å»ºå·¥ä½œçº¿ç¨‹"""
        # è®¾ç½®websocket
        self.websocket = websocket

        # åˆå§‹åŒ–
        await self.initialize(system_role_path)

        # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
        self.should_stop.clear()

        # åˆ›å»ºLLMå¤„ç†çº¿ç¨‹
        self.llm_thread = threading.Thread(target=lambda: asyncio.run(self.llm_worker()))
        self.llm_thread.daemon = True   # å®ˆæŠ¤çº¿ç¨‹ï¼šä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨ç»“æŸ
        self.llm_thread.start()

        # åˆ›å»ºéŸ³é¢‘æ’­æ”¾çº¿ç¨‹ï¼ˆå¦‚æœæ²¡æœ‰websocketçš„è¯ï¼‰ï¼›å¦‚æœæœ‰websocketå°±ä¼šåœ¨ç›´æ¥å°†éŸ³é¢‘å‘é€ç»™å®¢æˆ·ç«¯
        if not self.websocket:
            self.audio_thread = threading.Thread(target=lambda: asyncio.run(self.audio_worker()))
            self.audio_thread.daemon = True  # å®ˆæŠ¤çº¿ç¨‹ï¼šä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨ç»“æŸ
            self.audio_thread.start()

        print("ChatHandler å·²å¯åŠ¨")

    async def stop(self):
        """åœæ­¢å¤„ç†å™¨å’Œç›¸å…³çº¿ç¨‹"""
        self.should_stop.set()

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.llm_thread and self.llm_thread.is_alive():
            self.llm_thread.join(timeout=5)

        if self.audio_thread and self.audio_thread.is_alive():
            self.audio_thread.join(timeout=5)

        print("ChatHandler å·²åœæ­¢")


    async def chat_with_tts(self, user_input: str):
        """
        å¯¹è¯çš„é€»è¾‘å‡½æ•°ï¼š
        1.å°†ç”¨æˆ·è¾“å…¥æ”¾å…¥è¾“å…¥é˜Ÿåˆ—
        2.å¾ªç¯ä»æ¶ˆæ¯é˜Ÿåˆ—è¯»å–LLMçš„æµå¼å›å¤ï¼ˆå‘½ä»¤è¡Œè¾“å‡º + ä¾›éŸ³é¢‘è¾“å‡ºï¼‰
        3.æ„å»ºå®Œæ•´çš„å¥å­ï¼Œå°†å…¶è½¬æ¢ä¸ºéŸ³é¢‘å¹¶æ”¾å…¥éŸ³é¢‘é˜Ÿåˆ—ï¼Œä¾›éŸ³é¢‘æ’­æ”¾çº¿ç¨‹å¤„ç†ï¼›æˆ–è€…é€šè¿‡websocketå‘é€ç»™å®¢æˆ·ç«¯
        """
        if not self.tts_engine:
            raise ValueError("TTSæ¨¡å‹æœªåˆå§‹åŒ–")

        print("LLM: ", end="", flush=True)

        # å°†ç”¨æˆ·è¾“å…¥æ”¾å…¥è¾“å…¥é˜Ÿåˆ—
        self.input_queue.put(user_input)

        # ä»æ¶ˆæ¯é˜Ÿåˆ—è¯»å–LLMçš„æµå¼å›å¤
        message_buffer = ""

        while True:
            try:
                # è·å–ä¸€ä¸ªæ¶ˆæ¯ç‰‡æ®µ
                chunk = self.message_queue.get(timeout=10)

                # å¦‚æœæ”¶åˆ°Noneï¼Œè¡¨ç¤ºæµç»“æŸï¼ˆå½“llm_workeråœ¨å¤„ç†å®Œæ­¤è½®å¯¹è¯åä¼šå‘é€Noneï¼‰
                if chunk is None:
                    # å¤„ç†æœ€åå‰©ä½™çš„æ–‡æœ¬
                    if message_buffer:
                        audio_data = await self.tts_engine.text_to_speech(message_buffer)
                        await self._handle_audio_data(audio_data)

                    self.message_queue.task_done()
                    break

                # ç´¯ç§¯æ¶ˆæ¯
                message_buffer += chunk
                print(chunk, end="", flush=True)

                # å°è¯•æ‹†åˆ†å¥å­
                sentences, remaining = self.split_sentences(message_buffer)

                # å¤„ç†å®Œæ•´çš„å¥å­
                if sentences:
                    # ä¸ºæ¯ä¸ªå®Œæ•´çš„å¥å­ç”Ÿæˆè¯­éŸ³
                    for sentence in sentences:
                        audio_data = await self.tts_engine.text_to_speech(sentence)
                        await self._handle_audio_data(audio_data)

                # ä¿å­˜å‰©ä½™çš„ä¸å®Œæ•´å¥å­
                message_buffer = remaining

                self.message_queue.task_done()

            except queue.Empty:
                print("\n[è­¦å‘Š] ç­‰å¾…LLMå“åº”è¶…æ—¶")
                break

        print()  # æ¢è¡Œï¼Œä¿æŒè¾“å‡ºæ•´æ´

        # ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæ¯•ï¼ˆå³ä½¿ä¸€ç›´ä¸ºç©ºä¹Ÿèƒ½joinï¼‰
        self.audio_queue.join()

    async def _handle_audio_data(self, audio_data: bytes):
        """
        å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œå°†å…¶å‘é€åˆ°WebSocketæˆ–æ”¾å…¥éŸ³é¢‘é˜Ÿåˆ—
        å¦‚æœæœ‰WebSocketè¿æ¥ï¼Œåˆ™ç›´æ¥å‘é€éŸ³é¢‘æ•°æ®åˆ°å®¢æˆ·ç«¯
        å¦åˆ™ï¼Œå°†éŸ³é¢‘æ•°æ®æ”¾å…¥éŸ³é¢‘é˜Ÿåˆ—ä¾›éŸ³é¢‘æ’­æ”¾çº¿ç¨‹å¤„ç†
        """
        if audio_data:
            if self.websocket:
                # å¦‚æœæœ‰WebSocketè¿æ¥ï¼Œç›´æ¥å‘é€éŸ³é¢‘æ•°æ®
                await self.websocket.send_bytes(audio_data)
            else:
                # å¦åˆ™ï¼Œå°†éŸ³é¢‘æ•°æ®æ”¾å…¥éŸ³é¢‘é˜Ÿåˆ—
                self.audio_queue.put(audio_data)

    # äº¤äº’å¼å¯¹è¯å¾ªç¯-----------------------------------------------------------------------------------
    async def interactive_loop_with_tts(self):
        """äº¤äº’å¼å¯¹è¯å¾ªç¯ï¼Œå¸¦TTSåŠŸèƒ½"""
        try:
            while True:
                user_input = input("\nYou: ")
                if user_input.lower() in ["exit", "quit"]:
                    print("é€€å‡ºå¯¹è¯...")
                    break

                # await self.chat_with_tts(user_input)
                await self.chat_with_tts(user_input)
        finally:
            await self.stop()

    async def interactive_loop_with_tts_asr(self):
        """
        äº¤äº’å¼å¯¹è¯å¾ªç¯ï¼Œå¸¦ASRã€TTSåŠŸèƒ½
        """
        try:
            while True:
                # ä½¿ç”¨ASRå½•éŸ³å¹¶è½¬æ¢ä¸ºæ–‡æœ¬
                audio_file_path = self.audio_recorder.record_audio()
                print("--start audio -> text--")
                user_input = self.asr_engine.audio_to_text(audio_file_path)
                print(f"\nYou: {user_input}")
                # ä¸‹é¢æ“ä½œå¯¹è¯­éŸ³è¾“å…¥æ²¡ç”¨ï¼Œå¯¹é”®ç›˜è¾“å…¥æœ‰ç”¨
                if user_input.lower() in ["exit", "quit"]:
                    print("é€€å‡ºå¯¹è¯...")
                    break

                await self.chat_with_tts(user_input)

                # éœ€è¦æ¸…é™¤ä¸´æ—¶æ–‡ä»¶
                self.audio_recorder.cleanup()
        finally:
            await self.stop()

    # äº¤äº’å¼å•æ¬¡å¯¹è¯-----------------------------------------------------------------------------------
    async def interactive_with_audio_input(self, audio_file_path: str):
        """
        å•æ¬¡äº¤äº’å¼å¯¹è¯ï¼Œå¸¦éŸ³é¢‘è¾“å…¥ï¼Œç”¨äºå’Œå®¢æˆ·ç«¯äº¤äº’
        æœ€åä¸éœ€è¦stopä¸­æ­¢ï¼Œè€Œæ˜¯ç­‰åˆ°websocketæ–­å¼€åæ‰ä¸­æ­¢
        param:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        # ä½¿ç”¨ASRå°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬
        user_input = self.asr_engine.audio_to_text(audio_file_path)
        print(f"\nYou: {user_input}")

        await self.chat_with_tts(user_input)

    async def interactive_with_text_input(self, input_text: str):
        """
        å•æ¬¡äº¤äº’å¼å¯¹è¯ï¼Œå¸¦éŸ³é¢‘è¾“å…¥ï¼Œç”¨äºå’Œå®¢æˆ·ç«¯äº¤äº’
        æœ€åä¸éœ€è¦stopä¸­æ­¢ï¼Œè€Œæ˜¯ç­‰åˆ°websocketæ–­å¼€åæ‰ä¸­æ­¢
        param:
            input_text: è¾“å…¥æ–‡æœ¬
        """
        # ä½¿ç”¨ASRå°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬
        print(f"\nYou: {input_text}")

        await self.chat_with_tts(input_text)

        print("å¯¹è¯å·²å®Œæˆï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡è¾“å…¥...")



    # å·¥å…·ç›¸å…³------------------------------------------------------------------------------------------
    async def prepare_tools(self):
        """
        å‡†å¤‡å·¥å…·åˆ—è¡¨ï¼Œå°†MCPå®¢æˆ·ç«¯çš„å·¥å…·è½¬æ¢ä¸ºOpenAI APIæ‰€éœ€çš„æ ¼å¼
        å¹¶æ ¹æ®ç™½åå•è¿‡æ»¤å·¥å…·
        """
        tools = await self.mcp_client.client.list_tools()
        # æ ¹æ®ç™½åå•è¿‡æ»¤
        filtered_tools = [
            tool for tool in tools if self._is_tool_allowed(tool.name)
        ]
        print(f"æ€»å·¥å…·æ•°: {len(tools)}, è¿‡æ»¤åå·¥å…·æ•°: {len(filtered_tools)}")

        structured_tools = [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            }
            for tool in filtered_tools
        ]

        for tool in filtered_tools:
            print(f"tool: {tool}")

        return structured_tools

    def _load_tools_whitelist(self, whitelist_path):
        """åŠ è½½å·¥å…·ç™½åå•é…ç½®æ–‡ä»¶"""
        if not whitelist_path or not Path(whitelist_path).exists():
            return None

        with open(whitelist_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _is_tool_allowed(self, tool_name: str):
        """æ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨ç™½åå•ä¸­"""
        if not self.tools_whitelist:
            return True  # å¦‚æœæ²¡æœ‰ç™½åå•ï¼Œåˆ™å…è®¸æ‰€æœ‰å·¥å…·

        # æå–æœåŠ¡å™¨åç§°ï¼ˆå·¥å…·åç§°æ ¼å¼ä¸º "server-service_tool_name"ï¼‰
        #  æ ¹æ®ç¬¬ä¸€ä¸ª '_' æ¥åˆ†å‰²
        server_name, function_name = tool_name.split('_', maxsplit=1)

        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨ç™½åå•ä¸­ä¸”å·²å¯ç”¨
        if (server_name in self.tools_whitelist.get('mcp_servers', {}) and
                self.tools_whitelist['mcp_servers'][server_name].get('enabled', False)):

            # å¦‚æœå…è®¸æ‰€æœ‰
            if self.tools_whitelist['mcp_servers'][server_name].get('allow_all', False):
                return True

            # æ£€æŸ¥å…·ä½“å·¥å…·æ˜¯å¦åœ¨ç™½åå•ä¸­
            allowed_tools = self.tools_whitelist['mcp_servers'][server_name].get('tools', [])
            return function_name in allowed_tools

        return False

    async def _process_tool_calls(self, tool_calls):
        """
        å¤„ç†å·¥å…·è°ƒç”¨
        """
        for tool_call in tool_calls:
            print(f"  - Calling tool: {tool_call.function.name}")
            try:
                # ä½¿ç”¨MCPç®¡ç†å™¨æ‰§è¡Œè°ƒç”¨
                tool_result = await self.mcp_client.client.call_tool(
                    tool_call.function.name,
                    json.loads(tool_call.function.arguments)
                )

                print(f"âœ… Tool call successful: {tool_call.function.name}, arguments: {tool_call.function.arguments},  Result: {tool_result.content[0].text}")

                # å°†å·¥å…·è°ƒç”¨çš„ç»“æœæ·»åŠ å›å†å²è®°å½•
                self.history.append({
                    "role": "tool",
                    "content": tool_result.content[0].text,
                    "tool_call_id": tool_call.id
                })
            except Exception as e:
                # æ•è·å¼‚å¸¸å¹¶è®°å½•é”™è¯¯
                error_message = f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_call.function.name}, é”™è¯¯: {str(e)}"
                print(f"âŒ {error_message}")

                # å°†é”™è¯¯ä¿¡æ¯ä½œä¸ºå·¥å…·å“åº”æ·»åŠ åˆ°å†å²è®°å½•ä¸­
                self.history.append({
                    "role": "tool",
                    "content": error_message,
                    "tool_call_id": tool_call.id
                })
