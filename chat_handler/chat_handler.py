import json
import yaml
from pathlib import Path

from chat_handler.chat_context_manager import ChatContextManager

class ChatHandler:
    def __init__(self, openai_engine, mcp_client, whitelist_path=None, max_context_tokens=64000):
        self.llm = openai_engine
        self.mcp_client = mcp_client

        # å·¥å…·åˆ—è¡¨
        self.tools = None
        # å·¥å…·ç™½åå•é…ç½®
        self.tools_whitelist = self._load_tools_whitelist(whitelist_path)

        # ä¸Šä¸‹æ–‡
        self.history = []
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context_manager = ChatContextManager(
            llm_engine=openai_engine,
            max_context_tokens=max_context_tokens
        )
        # å½“å‰ç´¯ç§¯messageçš„tokensæ•°é‡ï¼ˆç›´æ¥ä»æœ¬è½®å¯¹è¯çš„responseçš„prompt_tokenä¸­è¯»å–ï¼‰
        self.message_tokens = 0
        # ç´¯ç§¯ä½¿ç”¨çš„tokensè®¡æ•°ï¼ˆå°†æ¯è½®å¯¹è¯çš„tokenséƒ½åŠ èµ·æ¥ï¼‰
        self.tokens_used = {"prompt_cached_tokens": 0, "prompt_miss_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0}

    async def initialize(self, system_role_path=None):
        """
        åˆå§‹åŒ–èŠå¤©å¤„ç†å™¨ï¼Œå‡†å¤‡å¿…è¦çš„å·¥å…·å’Œä¸Šä¸‹æ–‡ã€‚
        """
        # å‡†å¤‡å·¥å…·åˆ—è¡¨
        self.tools = await self.prepare_tools()
        # åˆå§‹åŒ–ç³»ç»Ÿæç¤º
        self.init_system_prompt(system_role_path)

    def init_system_prompt(self, system_role_path: str):
        if system_role_path:
            # åŠ è½½ YAML é…ç½®æ–‡ä»¶
            with open("chat_handler/system_role_prompt.yaml", "r") as file:
                config = yaml.safe_load(file)

            # è·å–è§’è‰²Açš„æè¿°
            role_description = config['roles']['SpongeBob']
            tools_description = config['tools']['mcp_tools']
            total_description = f"{role_description}\n{tools_description}"
            print(total_description)
            self.history.append({"role": "system", "content": total_description})


    async def handle_chat(self, user_input: str, use_stream=False):
        """
        è¿›è¡Œä¸€è½®å¯¹è¯ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è¿”å›LLMçš„æœ€ç»ˆå›å¤ã€‚
        å‚æ•°:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        """
        # 1. å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å†å²è®°å½•
        self.history.append({"role": "user", "content": user_input})

        # 2. è·å–å·¥å…·åˆ—è¡¨ï¼ˆåˆå§‹åŒ–çš„æ—¶å€™è·å–è¿‡ä¸€æ¬¡ï¼Œè¿™é‡Œä¸ºäº†ä¿é™©ï¼‰
        if not self.tools:
            self.tools = await self.prepare_tools()

        while True:
            # 3. è°ƒç”¨LLM API (æ ¹æ®use_streamå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼è°ƒç”¨)
            if use_stream:
                response_message, finish_reason, tokens_used = await self._call_llm_stream()
            else:
                response_message, finish_reason, tokens_used = await self._call_llm_normal()

            # æ›´æ–°tokenè®¡æ•°å™¨
            if tokens_used:
                self.message_tokens = getattr(tokens_used, 'prompt_tokens', 0)
                self.tokens_used['prompt_cached_tokens'] += getattr(tokens_used, 'prompt_cache_hit_tokens', 0)
                self.tokens_used['prompt_miss_tokens'] += getattr(tokens_used, 'prompt_cache_miss_tokens', 0)
                self.tokens_used['prompt_tokens'] += getattr(tokens_used, 'prompt_tokens', 0)
                self.tokens_used['completion_tokens'] += getattr(tokens_used, 'completion_tokens', 0)

            # è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯
            if tokens_used:
                print(
                    f"\nCurrent tokens used: [prompt_cached_tokens]: {getattr(tokens_used, 'prompt_cache_hit_tokens', 0)}"
                    f" - [prompt_miss_tokens]: {getattr(tokens_used, 'prompt_cache_miss_tokens', 0)}"
                    f" - [prompt_tokens]: {getattr(tokens_used, 'prompt_tokens', 0)}"
                    f" - [completion_tokens]: {getattr(tokens_used, 'completion_tokens', 0)}")
                print(f"Total tokens used: [prompt_cached_tokens]: {self.tokens_used['prompt_cached_tokens']}"
                      f" - [prompt_miss_tokens]: {self.tokens_used['prompt_miss_tokens']}"
                      f" - [prompt_tokens]: {self.tokens_used['prompt_tokens']}"
                      f" - [completion_tokens]: {self.tokens_used['completion_tokens']}")

            # 3.1 å¦‚æœLLMæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™ç›´æ¥è¿”å›ç»“æœ
            if finish_reason != "tool_calls":
                self.history.append({
                    "role": "assistant",
                    "content": response_message.content,
                })
                return response_message

            # 3.2 å¦åˆ™ï¼ŒLLMè¯·æ±‚å·¥å…·è°ƒç”¨
            print("ğŸ¤– LLM requested tool calls...")
            # i. å°†LLMçš„å›å¤æ·»åŠ åˆ°å†å²è®°å½•
            self.history.append({
                "role": "assistant",
                "content": response_message.content,
                # è¿™é‡Œä¸€å®šè¦æœ‰ tool_calls å­—æ®µï¼Œé‡Œé¢åŒ…å«äº†LLMè¯·æ±‚çš„æ‰€æœ‰å·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯ï¼Œ
                #  å¦åˆ™ä¸‹é¢è°ƒç”¨å®Œå·¥å…·æ”¾å…¥message çš„toolå­—æ®µä¸­ä¼šå‡ºé”™
                "tool_calls": response_message.tool_calls
            })

            # ii. æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            await self._process_tool_calls(response_message.tool_calls)

            # å¸¦ç€å·¥å…·è°ƒç”¨çš„ç»“æœå†æ¬¡è¯·æ±‚LLMè¿›è¡Œæ€»ç»“ï¼Œå¾ªç¯ç»§ç»­
            print("ğŸ”„ Sending tool results back to LLM for final response...")

    async def _call_llm_normal(self):
        """
        è°ƒç”¨LLM APIçš„æ ‡å‡†æ–¹å¼
        """
        response = await self.llm.chat(self.history, self.tools)
        response_message = response.choices[0].message
        finish_reason = response.choices[0].finish_reason
        return response_message, finish_reason, response.usage


    async def _call_llm_stream(self):
        """
        è°ƒç”¨LLM APIçš„æµå¼æ–¹å¼
        """
        stream = await self.llm.chat_stream(self.history, self.tools)

        # ç”¨äºç´¯ç§¯å®Œæ•´å“åº”
        response_content = ""
        tool_calls = []
        finish_reason = None
        tokens_used = None

        # å¤„ç†æµå¼å“åº”
        print("LLM: ", end="", flush=True)
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta:
                delta = chunk.choices[0].delta

                # æ”¶é›†å†…å®¹ç‰‡æ®µ
                if delta.content:
                    response_content += delta.content
                    print(delta.content, end="", flush=True)

                # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
                if delta.tool_calls:
                    for tool_call_delta in delta.tool_calls:
                        # æŸ¥æ‰¾æˆ–åˆ›å»ºå·¥å…·è°ƒç”¨
                        existing_call = next((tc for tc in tool_calls if tc.index == tool_call_delta.index), None)
                        if existing_call is None:
                            tool_calls.append(tool_call_delta)
                        else:
                            # æ›´æ–°ç°æœ‰å·¥å…·è°ƒç”¨
                            if tool_call_delta.function and tool_call_delta.function.name:
                                existing_call.function.name = tool_call_delta.function.name
                            if tool_call_delta.function and tool_call_delta.function.arguments:
                                existing_call.function.arguments = (
                                                                               existing_call.function.arguments or "") + tool_call_delta.function.arguments
                            if tool_call_delta.id:
                                existing_call.id = tool_call_delta.id

            # æ£€æŸ¥å®ŒæˆåŸå› 
            if chunk.choices and chunk.choices[0].finish_reason:
                finish_reason = chunk.choices[0].finish_reason
                tokens_used = chunk.usage

        print()  # æ¢è¡Œ

        # æ„é€ å“åº”æ¶ˆæ¯
        response_message = type('obj', (object,), {
            'content': response_content,
            'tool_calls': tool_calls if tool_calls else None
        })

        return response_message, finish_reason, tokens_used

    async def _process_tool_calls(self, tool_calls):
        """
        å¤„ç†å·¥å…·è°ƒç”¨
        """
        for tool_call in tool_calls:
            print(f"  - Calling tool: {tool_call.function.name}")
            try:
                # ä½¿ç”¨MCPç®¡ç†å™¨æ‰§è¡Œè°ƒç”¨
                tool_result = await self.mcp_client.client.call_tool(
                    tool_call.function.name,
                    json.loads(tool_call.function.arguments)
                )

                print(f"âœ… Tool call successful: {tool_call.function.name}, arguments: {tool_call.function.arguments},  Result: {tool_result.content[0].text}")

                # å°†å·¥å…·è°ƒç”¨çš„ç»“æœæ·»åŠ å›å†å²è®°å½•
                self.history.append({
                    "role": "tool",
                    "content": tool_result.content[0].text,
                    "tool_call_id": tool_call.id
                })
            except Exception as e:
                # æ•è·å¼‚å¸¸å¹¶è®°å½•é”™è¯¯
                error_message = f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_call.function.name}, é”™è¯¯: {str(e)}"
                print(f"âŒ {error_message}")

                # å°†é”™è¯¯ä¿¡æ¯ä½œä¸ºå·¥å…·å“åº”æ·»åŠ åˆ°å†å²è®°å½•ä¸­
                self.history.append({
                    "role": "tool",
                    "content": error_message,
                    "tool_call_id": tool_call.id
                })

    async def prepare_tools(self):
        """
        å‡†å¤‡å·¥å…·åˆ—è¡¨ï¼Œå°†MCPå®¢æˆ·ç«¯çš„å·¥å…·è½¬æ¢ä¸ºOpenAI APIæ‰€éœ€çš„æ ¼å¼
        å¹¶æ ¹æ®ç™½åå•è¿‡æ»¤å·¥å…·
        """
        tools = await self.mcp_client.client.list_tools()
        # æ ¹æ®ç™½åå•è¿‡æ»¤
        filtered_tools = [
            tool for tool in tools if self._is_tool_allowed(tool.name)
        ]
        print(f"æ€»å·¥å…·æ•°: {len(tools)}, è¿‡æ»¤åå·¥å…·æ•°: {len(filtered_tools)}")

        structured_tools = [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            }
            for tool in filtered_tools
        ]

        for tool in filtered_tools:
            print(f"tool: {tool}")

        return structured_tools

    def _load_tools_whitelist(self, whitelist_path):
        """åŠ è½½å·¥å…·ç™½åå•é…ç½®æ–‡ä»¶"""
        if not whitelist_path or not Path(whitelist_path).exists():
            return None

        with open(whitelist_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _is_tool_allowed(self, tool_name: str):
        """æ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨ç™½åå•ä¸­"""
        if not self.tools_whitelist:
            return True  # å¦‚æœæ²¡æœ‰ç™½åå•ï¼Œåˆ™å…è®¸æ‰€æœ‰å·¥å…·

        # æå–æœåŠ¡å™¨åç§°ï¼ˆå·¥å…·åç§°æ ¼å¼ä¸º "server-service_tool_name"ï¼‰
        #  æ ¹æ®ç¬¬ä¸€ä¸ª '_' æ¥åˆ†å‰²
        server_name, function_name = tool_name.split('_', maxsplit=1)

        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨ç™½åå•ä¸­ä¸”å·²å¯ç”¨
        if (server_name in self.tools_whitelist.get('mcp_servers', {}) and
                self.tools_whitelist['mcp_servers'][server_name].get('enabled', False)):

            # å¦‚æœå…è®¸æ‰€æœ‰
            if self.tools_whitelist['mcp_servers'][server_name].get('allow_all', False):
                return True

            # æ£€æŸ¥å…·ä½“å·¥å…·æ˜¯å¦åœ¨ç™½åå•ä¸­
            allowed_tools = self.tools_whitelist['mcp_servers'][server_name].get('tools', [])
            return function_name in allowed_tools

        return False

    async def loop(self, use_stream=False):
        """
        å¤„ç†å¯¹è¯å¾ªç¯ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥å¹¶è¿›è¡Œå¯¹è¯ï¼›
        ä¸€è½®å¯¹è¯ç»“æŸåï¼Œè‡ªåŠ¨ç²¾ç®€ä¸Šä¸‹æ–‡ä»¥ä¿æŒä¸Šä¸‹æ–‡çš„æœ‰æ•ˆæ€§å’Œç®€æ´æ€§ã€‚
        å‚æ•°:
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        """
        while True:
            user_input = input("You: ")
            if user_input.lower() in ["exit", "quit"]:
                print("Exiting chat...")
                break

            response = await self.handle_chat(user_input, use_stream=use_stream)

            # å¦‚æœä¸æ˜¯æµå¼æ¨¡å¼ï¼Œéœ€è¦æ‰“å°è¾“å‡º
            if not use_stream:
                print(f"LLM: {response.content}")

            # æœ¬è½®å¯¹è¯ç»“æŸåï¼Œç²¾ç®€ä¸Šä¸‹æ–‡
            self.history = await self.context_manager.manage_context(self.history, self.message_tokens)


    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæ¥çš„æ–¹æ³•
    async def handle_chat_stream(self, user_input: str):
        """
        è¿›è¡Œä¸€è½®å¯¹è¯ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è¿”å›LLMçš„æœ€ç»ˆå›å¤(æµå¼å¤„ç†)ã€‚
        """
        return await self.handle_chat(user_input, use_stream=True)

    async def loop_stream(self):
        """
        å¤„ç†å¯¹è¯å¾ªç¯ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥å¹¶è¿›è¡Œå¯¹è¯ï¼Œä½¿ç”¨æµå¼è¾“å‡ºã€‚
        """
        await self.loop(use_stream=True)